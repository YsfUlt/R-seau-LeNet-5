{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNe+tVQEwamS0H2Z8W1sYlG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"epEaUYnDyGpW","executionInfo":{"status":"ok","timestamp":1751520956154,"user_tz":-60,"elapsed":16,"user":{"displayName":"YOUSSEF ELMELH","userId":"11833059091909848828"}}},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import cv2\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","from sklearn.metrics import confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import time\n","from tqdm import tqdm\n","\n","# Fonctions d'activation\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_derivative(x):\n","    return (x > 0).astype(float)\n","\n","def softmax(x):\n","    max_x = np.max(x, axis=1, keepdims=True)\n","    exp_x = np.exp(x - max_x)\n","    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n","\n","\n","\n","class Conv2D:\n","    def __init__(self, in_channels, out_channels, kernel_size):\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","\n","        # Initialisation Glorot (Xavier)\n","        fan_in = in_channels * kernel_size * kernel_size\n","        fan_out = out_channels * kernel_size * kernel_size\n","        std = np.sqrt(2.0 / (fan_in + fan_out))\n","        self.weights = np.random.normal(0, std, (out_channels, in_channels, kernel_size, kernel_size))\n","        self.bias = np.zeros(out_channels)\n","        self.cache = None\n","\n","    def forward(self, x):\n","        batch_size, in_height, in_width, _ = x.shape\n","        out_height = in_height - self.kernel_size + 1\n","        out_width = in_width - self.kernel_size + 1\n","\n","        output = np.zeros((batch_size, out_height, out_width, self.out_channels))\n","\n","        for i in range(out_height):\n","            for j in range(out_width):\n","                region = x[:, i:i+self.kernel_size, j:j+self.kernel_size, :]\n","                region_reshaped = region.reshape((batch_size, -1))\n","                weights_reshaped = self.weights.reshape((self.out_channels, -1))\n","                output[:, i, j, :] = region_reshaped @ weights_reshaped.T + self.bias\n","\n","        self.cache = (x, (out_height, out_width))\n","        return output\n","\n","\n","    def backward(self, dout, lr):\n","        x, (out_height, out_width) = self.cache\n","        batch_size, in_height, in_width, in_channels = x.shape\n","\n","        dweights = np.zeros_like(self.weights)\n","        dbias = np.sum(dout, axis=(0, 1, 2))\n","        dx = np.zeros_like(x)\n","\n","        for i in range(out_height):\n","            i_start, i_end = i, i + self.kernel_size\n","            for j in range(out_width):\n","                j_start, j_end = j, j + self.kernel_size\n","\n","                region = x[:, i_start:i_end, j_start:j_end, :]\n","                dout_slice = dout[:, i:i+1, j:j+1, :]\n","\n","                # Correction : Calcul de dweights avec transposition\n","                for k in range(self.out_channels):\n","                    product = region * dout_slice[:, :, :, k:k+1]  # Broadcasting correct\n","                    temp = np.sum(product, axis=0)  # Forme (kernel_size, kernel_size, in_channels)\n","                    dweights[k] += temp.transpose(2, 0, 1)  # Transposée en (in_channels, kernel_size, kernel_size)\n","\n","                # Correction : Calcul vectorisé de dx\n","                dout_slice_flat = dout_slice.reshape(batch_size, -1)  # Forme (batch_size, out_channels)\n","                weights_flat = self.weights.transpose(1, 2, 3, 0).reshape(-1, self.out_channels)\n","                dx_region = np.dot(dout_slice_flat, weights_flat.T).reshape(\n","                    batch_size, self.kernel_size, self.kernel_size, in_channels\n","                )\n","                dx[:, i_start:i_end, j_start:j_end, :] += dx_region\n","\n","        # Mise à jour des paramètres\n","        self.weights -= lr * dweights / batch_size\n","        self.bias -= lr * dbias / batch_size\n","\n","        return dx\n","\n","\n","class AvgPool2D:\n","    def __init__(self, pool_size=2, stride=2):\n","        self.pool_size = pool_size\n","        self.stride = stride\n","        self.cache = None\n","\n","    def forward(self, x):\n","        batch_size, in_height, in_width, in_channels = x.shape\n","        out_height = (in_height - self.pool_size) // self.stride + 1\n","        out_width = (in_width - self.pool_size) // self.stride + 1\n","\n","        output = np.zeros((batch_size, out_height, out_width, in_channels))\n","\n","        for i in range(out_height):\n","            for j in range(out_width):\n","                h_start = i * self.stride\n","                h_end = h_start + self.pool_size\n","                w_start = j * self.stride\n","                w_end = w_start + self.pool_size\n","\n","                region = x[:, h_start:h_end, w_start:w_end, :]\n","                output[:, i, j, :] = np.mean(region, axis=(1, 2))\n","\n","        self.cache = (x.shape, region)\n","        return output\n","\n","    def backward(self, dout):\n","        input_shape, _ = self.cache\n","        batch_size, out_height, out_width, in_channels = dout.shape\n","        dx = np.zeros(input_shape)\n","        pool_area = self.pool_size * self.pool_size\n","\n","        for i in range(out_height):\n","            for j in range(out_width):\n","                h_start = i * self.stride\n","                h_end = h_start + self.pool_size\n","                w_start = j * self.stride\n","                w_end = w_start + self.pool_size\n","\n","                grad = dout[:, i:i+1, j:j+1, :] / pool_area\n","                dx[:, h_start:h_end, w_start:w_end, :] += grad\n","\n","        return dx\n","\n","class Flatten:\n","    def __init__(self):\n","        self.cache = None\n","\n","    def forward(self, x):\n","        self.cache = x.shape\n","        return x.reshape(x.shape[0], -1)\n","\n","    def backward(self, dout):\n","        return dout.reshape(self.cache)\n","\n","class Dense:\n","    def __init__(self, input_size, output_size):\n","        # Initialisation Glorot (Xavier)\n","        std = np.sqrt(2.0 / (input_size + output_size))\n","        self.weights = np.random.normal(0, std, (input_size, output_size))\n","        self.bias = np.zeros(output_size)\n","        self.cache = None\n","\n","    def forward(self, x):\n","        self.cache = x\n","        return np.dot(x, self.weights) + self.bias\n","\n","    def backward(self, dout, lr):\n","        x = self.cache\n","        batch_size = x.shape[0]\n","\n","        dw = np.dot(x.T, dout)\n","        db = np.sum(dout, axis=0)\n","        dx = np.dot(dout, self.weights.T)\n","\n","        self.weights -= lr * dw / batch_size\n","        self.bias -= lr * db / batch_size\n","\n","        return dx\n","\n","class ReLU:\n","    def __init__(self):\n","        self.cache = None\n","\n","    def forward(self, x):\n","        self.cache = x\n","        return relu(x)\n","\n","    def backward(self, dout):\n","        x = self.cache\n","        dx = dout * relu_derivative(x)\n","        return dx\n","\n","class Softmax:\n","    def __init__(self):\n","        self.cache = None\n","\n","    def forward(self, x):\n","        output = softmax(x)\n","        self.cache = output\n","        return output\n","\n","    def backward(self, dout, y_true):\n","        output = self.cache\n","        batch_size = y_true.shape[0]\n","        #dx = (output - y_true) / batch_size\n","        dx = (output - y_true)\n","\n","        return dx\n","\n","# ===========================================\n","# Architecture LeNet-5 améliorée\n","# ===========================================\n","class LeNet5:\n","    def __init__(self, input_shape=(32, 32, 1), num_classes=33):\n","\n","        self.layers = [\n","            Conv2D(input_shape[-1], 6, 5),  # C1: 6 filtres 5x5 -> 28x28x6\n","            ReLU(),\n","            AvgPool2D(2, 2),                # S2: Pooling -> 14x14x6\n","            Conv2D(6, 16, 5),               # C3: 16 filtres 5x5 -> 10x10x16\n","            ReLU(),\n","            AvgPool2D(2, 2),                # S4: Pooling -> 5x5x16\n","            Flatten(),\n","            Dense(16*5*5, 120),             # C5: Fully connected 120 neurones\n","            ReLU(),\n","            Dense(120, 84),                 # F6: Fully connected 84 neurones\n","            ReLU(),\n","            Dense(84, num_classes),         # Sortie: 33 classes\n","            Softmax()\n","        ]\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer.forward(x)\n","        return x\n","\n","    def backward(self, y_pred, y_true, lr):\n","        grad = self.layers[-1].backward(None, y_true)\n","        for layer in reversed(self.layers[:-1]):\n","            if isinstance(layer, (Conv2D, Dense)):\n","                grad = layer.backward(grad, lr)\n","            else:\n","                grad = layer.backward(grad)\n","        return grad\n","\n","    def predict(self, x):\n","        return self.forward(x).argmax(axis=1)\n","\n","# ===========================================\n","# Fonctions utilitaires pour l'entraînement\n","# ===========================================\n","def cross_entropy_loss(y_pred, y_true):\n","    m = y_true.shape[0]\n","    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n","    log_likelihood = -np.sum(y_true * np.log(y_pred)) / m\n","    return log_likelihood\n","    #correct_logprobs = -np.log(y_pred[np.arange(m), y_true.argmax(axis=1)])\n","    #return np.sum(correct_logprobs) / m\n","\n","def accuracy(y_pred, y_true):\n","    return np.mean(y_pred.argmax(axis=1) == y_true.argmax(axis=1))\n","\n","def train_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=32, lr=0.01):\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    n_batches = int(np.ceil(X_train.shape[0] / batch_size))\n","\n","    for epoch in range(epochs):\n","        start_time = time.time()\n","        epoch_loss = 0\n","        epoch_acc = 0\n","\n","        # Mélanger les données\n","        indices = np.arange(X_train.shape[0])\n","        np.random.shuffle(indices)\n","        X_train_shuffled = X_train[indices]\n","        y_train_shuffled = y_train[indices]\n","\n","        # Barre de progression pour les batches\n","        for i in tqdm(range(n_batches), desc=f\"Epoch {epoch+1}/{epochs}\"):\n","            start = i * batch_size\n","            end = min((i+1) * batch_size, X_train.shape[0])\n","            X_batch = X_train_shuffled[start:end]\n","            y_batch = y_train_shuffled[start:end]\n","\n","            # Propagation avant\n","            y_pred = model.forward(X_batch)\n","\n","            # Calcul de la perte et précision\n","            loss = cross_entropy_loss(y_pred, y_batch)\n","            acc = accuracy(y_pred, y_batch)\n","\n","            # Rétropropagation\n","            model.backward(y_pred, y_batch, lr)\n","\n","            epoch_loss += loss\n","            epoch_acc += acc\n","\n","        # Calcul des moyennes\n","        epoch_loss /= n_batches\n","        epoch_acc /= n_batches\n","\n","        # Validation\n","        val_pred = model.forward(X_val)\n","        val_loss = cross_entropy_loss(val_pred, y_val)\n","        val_acc = accuracy(val_pred, y_val)\n","\n","        # Stockage des résultats\n","        train_losses.append(epoch_loss)\n","        train_accuracies.append(epoch_acc)\n","        val_losses.append(val_loss)\n","        val_accuracies.append(val_acc)\n","\n","        epoch_time = time.time() - start_time\n","        print(f\"Epoch {epoch+1}/{epochs} - {epoch_time:.2f}s - \"\n","              f\"Train Loss: {epoch_loss:.4f} - Train Acc: {epoch_acc:.4f} - \"\n","              f\"Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n","\n","    return train_losses, val_losses, train_accuracies, val_accuracies\n","\n","# ===========================================\n","# Chargement et prétraitement des données\n","# ===========================================\n","def load_and_preprocess_data(data_dir):\n","    csv_path = os.path.join(data_dir, 'labels-map.csv')\n","\n","    try:\n","        labels_df = pd.read_csv(csv_path, header=None, names=['image_path', 'label'])\n","        print(\"labels-map.csv found. Loading data from CSV...\")\n","\n","        # labels_df['image_path'] = labels_df['image_path'].str.replace(\n","        #     './images-data-64/tifinagh-images/',\n","        #     'tifinagh-images/',\n","        #     regex=False\n","        # )\n","        labels_df['image_path'] = labels_df['image_path'].str.replace(\n","            './images-data-64/tifinagh-images/',\n","            '',  # Supprimer complètement le préfixe erroné\n","            regex=False\n","        )\n","        labels_df['image_path'] = os.path.join(data_dir, 'tifinagh-images') + '/' + labels_df['image_path']\n","\n","\n","        labels_df['image_path'] = labels_df['image_path'].str.replace('./', '', regex=False)\n","        labels_df['image_path'] = labels_df['image_path'].apply(\n","            lambda x: os.path.join(data_dir, x)\n","        )\n","    except FileNotFoundError:\n","        print(\"labels-map.csv not found. Building DataFrame from folders...\")\n","        base_dir = os.path.join(data_dir, 'tifinagh-images')\n","        image_paths = []\n","        labels = []\n","        for label_dir in os.listdir(base_dir):\n","            label_path = os.path.join(base_dir, label_dir)\n","            if os.path.isdir(label_path):\n","                for img_name in os.listdir(label_path):\n","                    if img_name.lower().endswith(('.jpeg', '.jpg', '.png')):\n","                        image_paths.append(os.path.join(label_path, img_name))\n","                        labels.append(label_dir)\n","        labels_df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n","\n","    assert not labels_df.empty, \"No data loaded. Check dataset files.\"\n","    print(f\"Loaded {len(labels_df)} samples with {labels_df['label'].nunique()} unique classes.\")\n","\n","    label_encoder = LabelEncoder()\n","    labels_df['label_encoded'] = label_encoder.fit_transform(labels_df['label'])\n","    num_classes = len(label_encoder.classes_)\n","\n","\n","    def load_and_preprocess_image(image_path, target_size=(32, 32)):\n","      try:\n","          # Vérification renforcée de l'existence du fichier\n","          if not os.path.exists(image_path):\n","              # Tentative de récupération alternative\n","              alt_path = image_path.replace('tifinagh-images/', '')\n","              if os.path.exists(alt_path):\n","                  image_path = alt_path\n","                  print(f\"Info: Using alternative path - {alt_path}\")\n","              else:\n","                  print(f\"Warning: File not found - {image_path}\")\n","                  return None\n","\n","          # Chargement de l'image\n","          img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","          if img is None:\n","              print(f\"Warning: Failed to load image - {image_path}\")\n","              return None\n","\n","          # Prétraitement de l'image\n","          img = cv2.resize(img, target_size)\n","          img = img.astype(np.float32) / 255.0\n","\n","          # Normalisation: soustraction de la moyenne et division par l'écart-type\n","          img_mean = np.mean(img)\n","          img_std = np.std(img)\n","\n","          # Éviter la division par zéro\n","          if img_std < 1e-7:\n","              img_std = 1.0\n","\n","          img = (img - img_mean) / img_std\n","          return img.reshape(32, 32, 1)\n","\n","      except Exception as e:\n","          print(f\"Error processing image {image_path}: {str(e)}\")\n","          return None\n","\n","    images = []\n","    valid_indices = []\n","\n","    for idx, path in enumerate(labels_df['image_path']):\n","        img = load_and_preprocess_image(path)\n","        if img is not None:\n","            images.append(img)\n","            valid_indices.append(idx)\n","\n","    valid_df = labels_df.iloc[valid_indices]\n","    X = np.array(images)\n","    y = valid_df['label_encoded'].values\n","\n","    assert X.shape[0] == y.shape[0], \"Mismatch between number of images and labels\"\n","    assert X.shape[1:] == (32, 32, 1), f\"Expected image shape (32,32,1), got {X.shape[1:]}\"\n","\n","    # Division stratifiée des données\n","    X_temp, X_test, y_temp, y_test = train_test_split(\n","        X, y, test_size=0.2, stratify=y, random_state=42\n","    )\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42\n","    )\n","\n","    one_hot_encoder = OneHotEncoder(sparse_output=False)\n","    y_train_one_hot = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\n","    y_val_one_hot = one_hot_encoder.transform(y_val.reshape(-1, 1))\n","    y_test_one_hot = one_hot_encoder.transform(y_test.reshape(-1, 1))\n","\n","    return X_train, X_val, X_test, y_train_one_hot, y_val_one_hot, y_test_one_hot, num_classes, label_encoder\n","\n","# ===========================================\n","# Visualisation des résultats\n","# ===========================================\n","def plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies):\n","    plt.figure(figsize=(15, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.title('Loss Curve')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_accuracies, label='Train Accuracy')\n","    plt.plot(val_accuracies, label='Validation Accuracy')\n","    plt.title('Accuracy Curve')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig('lenet5_training_curves.png')\n","    plt.show()\n","\n","def plot_confusion_matrix(y_true, y_pred, class_names):\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(15, 12))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=class_names,\n","                yticklabels=class_names)\n","    plt.title('Confusion Matrix (Test set)')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.xticks(rotation=45)\n","    plt.yticks(rotation=0)\n","    plt.tight_layout()\n","    plt.savefig('lenet5_confusion_matrix.png')\n","    plt.show()\n","\n","# ===========================================\n","# Point d'entrée principal\n","# ===========================================\n","if __name__ == \"__main__\":\n","    #data_dir = os.path.join(os.getcwd(), 'amhcd-data-64')\n","    data_dir = os.path.join(os.getcwd(), 'amhcd-data/amhcd-data-64')\n","\n","\n","\n","\n","\n","    # Charger les données\n","    X_train, X_val, X_test, y_train, y_val, y_test, num_classes, label_encoder = load_and_preprocess_data(data_dir)\n","\n","    print(f\"Train: {X_train.shape[0]} samples, Validation: {X_val.shape[0]} samples, Test: {X_test.shape[0]} samples\")\n","    print(f\"Image shape: {X_train.shape[1:]}, Number of classes: {num_classes}\")\n","\n","    # Réduction stratifiée pour le débogage (optionnel)\n","    # from sklearn.model_selection import train_test_split\n","    # _, X_train, _, y_train = train_test_split(X_train, y_train, train_size=2000, stratify=np.argmax(y_train, axis=1), random_state=42)\n","    # _, X_val, _, y_val = train_test_split(X_val, y_val, train_size=500, stratify=np.argmax(y_val, axis=1), random_state=42)\n","\n","    # Initialiser le modèle\n","    model = LeNet5(input_shape=(32, 32, 1), num_classes=num_classes)\n","\n","    # Entraîner le modèle avec un learning rate adaptatif\n","    train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n","        model, X_train, y_train, X_val, y_val,\n","        epochs=20,\n","        batch_size=64,\n","        lr=0.001\n","    )\n","\n","    # Visualiser les courbes\n","    plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies)\n","\n","    # Évaluation sur le test set\n","    print(\"Evaluating on test set...\")\n","    y_pred_probs = model.forward(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","    y_true = np.argmax(y_test, axis=1)\n","\n","    test_acc = accuracy(y_pred_probs, y_test)\n","    print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n","\n","    # Matrice de confusion\n","    plot_confusion_matrix(y_true, y_pred, label_encoder.classes_)\n","\n","    # Rapport de classification\n","    print(\"\\nClassification Report (Test set):\")\n","    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n","\n","    # Visualisation des feature maps\n","    try:\n","        sample_idx = np.random.randint(0, len(X_test))\n","        sample_img = X_test[sample_idx]\n","\n","        activations = sample_img[np.newaxis, ...]\n","        feature_maps = model.layers[0].forward(activations)\n","\n","        plt.figure(figsize=(12, 6))\n","        plt.suptitle(\"Feature Maps: First Convolutional Layer\", fontsize=16)\n","        for i in range(min(16, feature_maps.shape[-1])):\n","            plt.subplot(4, 4, i+1)\n","            plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n","            plt.axis('off')\n","        plt.tight_layout()\n","        plt.savefig('lenet5_feature_maps.png')\n","        plt.show()\n","    except Exception as e:\n","        print(f\"Feature map visualization skipped: {e}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"0ebcjQ9SyJka","executionInfo":{"status":"ok","timestamp":1751520969207,"user_tz":-60,"elapsed":6,"user":{"displayName":"YOUSSEF ELMELH","userId":"11833059091909848828"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##code 2"],"metadata":{"id":"9MWOLHu0yQlG","executionInfo":{"status":"ok","timestamp":1751521005639,"user_tz":-60,"elapsed":15,"user":{"displayName":"YOUSSEF ELMELH","userId":"11833059091909848828"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!pip uninstall -y cupy-cuda11x cupy-cuda12x\n","!pip install cupy-cuda12x\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import cupy as cp\n","import cv2\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","from sklearn.metrics import confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import time\n","from tqdm import tqdm\n","\n","# Fonctions d'activation\n","def relu(x):\n","    return cp.maximum(0, x)\n","\n","def relu_derivative(x):\n","    return (x > 0).astype(cp.float32)\n","\n","def softmax(x):\n","    max_x = cp.max(x, axis=1, keepdims=True)\n","    exp_x = cp.exp(x - max_x)\n","    return exp_x / cp.sum(exp_x, axis=1, keepdims=True)\n","\n","# Classes des couches\n","class Conv2D:\n","    def __init__(self, in_channels, out_channels, kernel_size):\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","\n","        # Initialisation avec CuPy\n","        fan_in = in_channels * kernel_size * kernel_size\n","        fan_out = out_channels * kernel_size * kernel_size\n","        std = cp.sqrt(2.0 / (fan_in + fan_out))\n","        self.weights = cp.random.normal(0, std, (out_channels, in_channels, kernel_size, kernel_size))\n","        self.bias = cp.zeros(out_channels)\n","        self.cache = None\n","\n","    def forward(self, x):\n","        batch_size, in_height, in_width, _ = x.shape\n","        out_height = in_height - self.kernel_size + 1\n","        out_width = in_width - self.kernel_size + 1\n","\n","        output = cp.zeros((batch_size, out_height, out_width, self.out_channels))\n","\n","        for i in range(out_height):\n","            for j in range(out_width):\n","                region = x[:, i:i+self.kernel_size, j:j+self.kernel_size, :]\n","                region_reshaped = region.reshape((batch_size, -1))\n","                weights_reshaped = self.weights.reshape((self.out_channels, -1))\n","                output[:, i, j, :] = region_reshaped @ weights_reshaped.T + self.bias\n","\n","        self.cache = (x, (out_height, out_width))\n","        return output\n","\n","    def backward(self, dout, lr):\n","        x, (out_height, out_width) = self.cache\n","        batch_size, in_height, in_width, in_channels = x.shape\n","\n","        dweights = cp.zeros_like(self.weights)\n","        dbias = cp.sum(dout, axis=(0, 1, 2))\n","        dx = cp.zeros_like(x)\n","\n","        for i in range(out_height):\n","            i_start, i_end = i, i + self.kernel_size\n","            for j in range(out_width):\n","                j_start, j_end = j, j + self.kernel_size\n","\n","                region = x[:, i_start:i_end, j_start:j_end, :]\n","                dout_slice = dout[:, i:i+1, j:j+1, :]\n","\n","                for k in range(self.out_channels):\n","                    product = region * dout_slice[:, :, :, k:k+1]\n","                    temp = cp.sum(product, axis=0)\n","                    dweights[k] += temp.transpose(2, 0, 1)\n","\n","                dout_slice_flat = dout_slice.reshape(batch_size, -1)\n","                weights_flat = self.weights.transpose(1, 2, 3, 0).reshape(-1, self.out_channels)\n","                dx_region = dout_slice_flat @ weights_flat.T\n","                dx_region = dx_region.reshape(batch_size, self.kernel_size, self.kernel_size, in_channels)\n","                dx[:, i_start:i_end, j_start:j_end, :] += dx_region\n","\n","        self.weights -= lr * dweights / batch_size\n","        self.bias -= lr * dbias / batch_size\n","\n","        return dx\n","\n","class AvgPool2D:\n","    def __init__(self, pool_size=2, stride=2):\n","        self.pool_size = pool_size\n","        self.stride = stride\n","        self.cache = None\n","\n","    def forward(self, x):\n","        batch_size, in_height, in_width, in_channels = x.shape\n","        out_height = (in_height - self.pool_size) // self.stride + 1\n","        out_width = (in_width - self.pool_size) // self.stride + 1\n","\n","        output = cp.zeros((batch_size, out_height, out_width, in_channels))\n","\n","        for i in range(out_height):\n","            for j in range(out_width):\n","                h_start = i * self.stride\n","                h_end = h_start + self.pool_size\n","                w_start = j * self.stride\n","                w_end = w_start + self.pool_size\n","\n","                region = x[:, h_start:h_end, w_start:w_end, :]\n","                output[:, i, j, :] = cp.mean(region, axis=(1, 2))\n","\n","        self.cache = (x.shape, region)\n","        return output\n","\n","    def backward(self, dout):\n","        input_shape, _ = self.cache\n","        batch_size, out_height, out_width, in_channels = dout.shape\n","        dx = cp.zeros(input_shape)\n","        pool_area = self.pool_size * self.pool_size\n","\n","        for i in range(out_height):\n","            for j in range(out_width):\n","                h_start = i * self.stride\n","                h_end = h_start + self.pool_size\n","                w_start = j * self.stride\n","                w_end = w_start + self.pool_size\n","\n","                grad = dout[:, i:i+1, j:j+1, :] / pool_area\n","                dx[:, h_start:h_end, w_start:w_end, :] += grad\n","\n","        return dx\n","\n","class Flatten:\n","    def __init__(self):\n","        self.cache = None\n","\n","    def forward(self, x):\n","        self.cache = x.shape\n","        return x.reshape(x.shape[0], -1)\n","\n","    def backward(self, dout):\n","        return dout.reshape(self.cache)\n","\n","class Dense:\n","    def __init__(self, input_size, output_size):\n","        std = cp.sqrt(2.0 / (input_size + output_size))\n","        self.weights = cp.random.normal(0, std, (input_size, output_size))\n","        self.bias = cp.zeros(output_size)\n","        self.cache = None\n","\n","    def forward(self, x):\n","        self.cache = x\n","        return cp.dot(x, self.weights) + self.bias\n","\n","    def backward(self, dout, lr):\n","        x = self.cache\n","        batch_size = x.shape[0]\n","\n","        dw = cp.dot(x.T, dout)\n","        db = cp.sum(dout, axis=0)\n","        dx = cp.dot(dout, self.weights.T)\n","\n","        self.weights -= lr * dw / batch_size\n","        self.bias -= lr * db / batch_size\n","\n","        return dx\n","\n","class ReLU:\n","    def __init__(self):\n","        self.cache = None\n","\n","    def forward(self, x):\n","        self.cache = x\n","        return relu(x)\n","\n","    def backward(self, dout):\n","        x = self.cache\n","        dx = dout * relu_derivative(x)\n","        return dx\n","\n","class Softmax:\n","    def __init__(self):\n","        self.cache = None\n","\n","    def forward(self, x):\n","        output = softmax(x)\n","        self.cache = output\n","        return output\n","\n","    def backward(self, dout, y_true):\n","        output = self.cache\n","        batch_size = y_true.shape[0]\n","        dx = (output - y_true)\n","        return dx\n","\n","# Architecture LeNet-5\n","class LeNet5:\n","    def __init__(self, input_shape=(32, 32, 1), num_classes=33):\n","        self.layers = [\n","            Conv2D(input_shape[-1], 6, 5),\n","            ReLU(),\n","            AvgPool2D(2, 2),\n","            Conv2D(6, 16, 5),\n","            ReLU(),\n","            AvgPool2D(2, 2),\n","            Flatten(),\n","            Dense(16*5*5, 120),\n","            ReLU(),\n","            Dense(120, 84),\n","            ReLU(),\n","            Dense(84, num_classes),\n","            Softmax()\n","        ]\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer.forward(x)\n","        return x\n","\n","    def backward(self, y_pred, y_true, lr):\n","        grad = self.layers[-1].backward(None, y_true)\n","        for layer in reversed(self.layers[:-1]):\n","            if isinstance(layer, (Conv2D, Dense)):\n","                grad = layer.backward(grad, lr)\n","            else:\n","                grad = layer.backward(grad)\n","        return grad\n","\n","    def predict(self, x):\n","        return self.forward(x).argmax(axis=1)\n","\n","# Fonctions utilitaires\n","def cross_entropy_loss(y_pred, y_true):\n","    m = y_true.shape[0]\n","    y_pred = cp.clip(y_pred, 1e-15, 1-1e-15)\n","    log_likelihood = -cp.sum(y_true * cp.log(y_pred)) / m\n","    return log_likelihood\n","\n","def accuracy(y_pred, y_true):\n","    return cp.mean(y_pred.argmax(axis=1) == y_true.argmax(axis=1))\n","\n","def train_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=32, lr=0.01):\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    n_batches = int(cp.ceil(X_train.shape[0] / batch_size))\n","\n","    for epoch in range(epochs):\n","        start_time = time.time()\n","        epoch_loss = 0\n","        epoch_acc = 0\n","\n","        indices = cp.arange(X_train.shape[0])\n","        cp.random.shuffle(indices)\n","        X_train_shuffled = X_train[indices]\n","        y_train_shuffled = y_train[indices]\n","\n","        for i in tqdm(range(n_batches), desc=f\"Epoch {epoch+1}/{epochs}\"):\n","            start = i * batch_size\n","            end = min((i+1) * batch_size, X_train.shape[0])\n","            X_batch = X_train_shuffled[start:end]\n","            y_batch = y_train_shuffled[start:end]\n","\n","            y_pred = model.forward(X_batch)\n","\n","            loss = cross_entropy_loss(y_pred, y_batch)\n","            acc = accuracy(y_pred, y_batch)\n","\n","            model.backward(y_pred, y_batch, lr)\n","\n","            epoch_loss += loss\n","            epoch_acc += acc\n","\n","        epoch_loss /= n_batches\n","        epoch_acc /= n_batches\n","\n","        val_pred = model.forward(X_val)\n","        val_loss = cross_entropy_loss(val_pred, y_val)\n","        val_acc = accuracy(val_pred, y_val)\n","\n","        train_losses.append(float(epoch_loss))\n","        train_accuracies.append(float(epoch_acc))\n","        val_losses.append(float(val_loss))\n","        val_accuracies.append(float(val_acc))\n","\n","        epoch_time = time.time() - start_time\n","        print(f\"Epoch {epoch+1}/{epochs} - {epoch_time:.2f}s - \"\n","              f\"Train Loss: {float(epoch_loss):.4f} - Train Acc: {float(epoch_acc):.4f} - \"\n","              f\"Val Loss: {float(val_loss):.4f} - Val Acc: {float(val_acc):.4f}\")\n","\n","    return train_losses, val_losses, train_accuracies, val_accuracies\n","\n","# Chargement des données\n","def load_and_preprocess_data(data_dir):\n","    csv_path = os.path.join(data_dir, 'labels-map.csv')\n","\n","    try:\n","        labels_df = pd.read_csv(csv_path, header=None, names=['image_path', 'label'])\n","        print(\"labels-map.csv found. Loading data from CSV...\")\n","\n","        labels_df['image_path'] = labels_df['image_path'].str.replace(\n","            './images-data-64/tifinagh-images/',\n","            '',\n","            regex=False\n","        )\n","        labels_df['image_path'] = os.path.join(data_dir, 'tifinagh-images') + '/' + labels_df['image_path']\n","        labels_df['image_path'] = labels_df['image_path'].str.replace('./', '', regex=False)\n","    except FileNotFoundError:\n","        print(\"labels-map.csv not found. Building DataFrame from folders...\")\n","        base_dir = os.path.join(data_dir, 'tifinagh-images')\n","\n","        # Vérification de l'existence du dossier\n","        if not os.path.exists(base_dir):\n","            print(f\"Error: Directory '{base_dir}' does not exist!\")\n","            print(f\"Available directories in {data_dir}: {os.listdir(data_dir)}\")\n","            raise FileNotFoundError(f\"Directory '{base_dir}' not found\")\n","\n","        image_paths = []\n","        labels = []\n","        for label_dir in os.listdir(base_dir):\n","            label_path = os.path.join(base_dir, label_dir)\n","            if os.path.isdir(label_path):\n","                for img_name in os.listdir(label_path):\n","                    if img_name.lower().endswith(('.jpeg', '.jpg', '.png')):\n","                        image_paths.append(os.path.join(label_path, img_name))\n","                        labels.append(label_dir)\n","        labels_df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n","\n","    assert not labels_df.empty, \"No data loaded. Check dataset files.\"\n","    print(f\"Loaded {len(labels_df)} samples with {labels_df['label'].nunique()} unique classes.\")\n","\n","    label_encoder = LabelEncoder()\n","    labels_df['label_encoded'] = label_encoder.fit_transform(labels_df['label'])\n","    num_classes = len(label_encoder.classes_)\n","\n","    def load_and_preprocess_image(image_path, target_size=(32, 32)):\n","        try:\n","            if not os.path.exists(image_path):\n","                alt_path = image_path.replace('tifinagh-images/', '')\n","                if os.path.exists(alt_path):\n","                    image_path = alt_path\n","                else:\n","                    print(f\"Warning: File not found - {image_path}\")\n","                    return None\n","\n","            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","            if img is None:\n","                print(f\"Warning: Failed to load image - {image_path}\")\n","                return None\n","\n","            img = cv2.resize(img, target_size)\n","            img = img.astype(np.float32) / 255.0\n","\n","            img_mean = np.mean(img)\n","            img_std = np.std(img)\n","            if img_std < 1e-7:\n","                img_std = 1.0\n","\n","            img = (img - img_mean) / img_std\n","            return img.reshape(32, 32, 1)\n","\n","        except Exception as e:\n","            print(f\"Error processing image {image_path}: {str(e)}\")\n","            return None\n","\n","    images = []\n","    valid_indices = []\n","\n","    for idx, path in enumerate(labels_df['image_path']):\n","        img = load_and_preprocess_image(path)\n","        if img is not None:\n","            images.append(img)\n","            valid_indices.append(idx)\n","\n","    valid_df = labels_df.iloc[valid_indices]\n","    X = np.array(images)\n","    y = valid_df['label_encoded'].values\n","\n","    assert X.shape[0] == y.shape[0], \"Mismatch between number of images and labels\"\n","    assert X.shape[1:] == (32, 32, 1), f\"Expected image shape (32,32,1), got {X.shape[1:]}\"\n","\n","    X_temp, X_test, y_temp, y_test = train_test_split(\n","        X, y, test_size=0.2, stratify=y, random_state=42\n","    )\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42\n","    )\n","\n","    one_hot_encoder = OneHotEncoder(sparse_output=False)\n","    y_train_one_hot = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\n","    y_val_one_hot = one_hot_encoder.transform(y_val.reshape(-1, 1))\n","    y_test_one_hot = one_hot_encoder.transform(y_test.reshape(-1, 1))\n","\n","    X_train = cp.asarray(X_train)\n","    X_val = cp.asarray(X_val)\n","    X_test = cp.asarray(X_test)\n","    y_train_one_hot = cp.asarray(y_train_one_hot)\n","    y_val_one_hot = cp.asarray(y_val_one_hot)\n","    y_test_one_hot = cp.asarray(y_test_one_hot)\n","\n","    return X_train, X_val, X_test, y_train_one_hot, y_val_one_hot, y_test_one_hot, num_classes, label_encoder\n","\n","# Visualisation\n","def plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies):\n","    plt.figure(figsize=(15, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.title('Loss Curve')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_accuracies, label='Train Accuracy')\n","    plt.plot(val_accuracies, label='Validation Accuracy')\n","    plt.title('Accuracy Curve')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig('lenet5_training_curves.png')\n","    plt.show()\n","\n","def plot_confusion_matrix(y_true, y_pred, class_names):\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(15, 12))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=class_names,\n","                yticklabels=class_names)\n","    plt.title('Confusion Matrix (Test set)')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.xticks(rotation=45)\n","    plt.yticks(rotation=0)\n","    plt.tight_layout()\n","    plt.savefig('lenet5_confusion_matrix.png')\n","    plt.show()\n","\n","# Point d'entrée principal\n","if __name__ == \"__main__\":\n","\n","\n","    data_dir = '/content/amhcd-data/amhcd-data-64'\n","\n","    # Vérifiez si le dossier existe\n","    if not os.path.exists(data_dir):\n","        print(f\"Data directory not found: {data_dir}\")\n","        print(\"Available directories in /content/drive/MyDrive:\")\n","        print(os.listdir('/content/drive/MyDrive'))\n","        raise FileNotFoundError(f\"Directory not found: {data_dir}\")\n","\n","    print(f\"Loading data from: {data_dir}\")\n","    print(f\"Directory contents: {os.listdir(data_dir)}\")\n","\n","    # Charger les données\n","    X_train, X_val, X_test, y_train, y_val, y_test, num_classes, label_encoder = load_and_preprocess_data(data_dir)\n","\n","    print(f\"Train: {X_train.shape[0]} samples, Validation: {X_val.shape[0]} samples, Test: {X_test.shape[0]} samples\")\n","    print(f\"Image shape: {X_train.shape[1:]}, Number of classes: {num_classes}\")\n","\n","    # Initialiser le modèle\n","    model = LeNet5(input_shape=(32, 32, 1), num_classes=num_classes)\n","\n","    # Entraîner le modèle\n","    train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n","        model, X_train, y_train, X_val, y_val,\n","        epochs=20,\n","        batch_size=64,\n","        lr=0.01\n","    )\n","\n","    # Visualiser les courbes\n","    plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies)\n","\n","    # Évaluation sur le test set\n","    print(\"Evaluating on test set...\")\n","    y_pred_probs = model.forward(X_test)\n","    y_pred = y_pred_probs.argmax(axis=1).get()\n","    y_true = y_test.argmax(axis=1).get()\n","\n","    test_acc = (y_pred == y_true).mean()\n","    print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n","\n","    # Matrice de confusion\n","    plot_confusion_matrix(y_true, y_pred, label_encoder.classes_)\n","\n","    # Rapport de classification\n","    print(\"\\nClassification Report (Test set):\")\n","    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n","\n","    # Visualisation des feature maps\n","    try:\n","        sample_idx = cp.random.randint(0, len(X_test))\n","        sample_img = X_test[sample_idx: sample_idx+1]\n","\n","        activations = sample_img\n","        feature_maps = model.layers[0].forward(activations).get()\n","\n","        plt.figure(figsize=(12, 6))\n","        plt.suptitle(\"Feature Maps: First Convolutional Layer\", fontsize=16)\n","        for i in range(min(16, feature_maps.shape[-1])):\n","            plt.subplot(4, 4, i+1)\n","            plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n","            plt.axis('off')\n","        plt.tight_layout()\n","        plt.savefig('lenet5_feature_maps.png')\n","        plt.show()\n","    except Exception as e:\n","        print(f\"Feature map visualization skipped: {e}\")"],"metadata":{"id":"_TtDH5t1yQq4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"daGb2jBcyQwv","executionInfo":{"status":"ok","timestamp":1751521041230,"user_tz":-60,"elapsed":18,"user":{"displayName":"YOUSSEF ELMELH","userId":"11833059091909848828"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9A4SLqUByQ0m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_BLu1W9iypMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##pour voir drive !\n","# Remplacer cette partie\n","if __name__ == \"__main__\":\n","    drive.mount('/content/drive')\n","\n","    # Afficher la structure des dossiers\n","    print(\"Contenu de /content/drive/MyDrive :\")\n","    print(os.listdir('/content/drive/MyDrive'))\n","\n","    # Trouver le bon chemin\n","    data_dir = None\n","    possible_paths = [\n","        '/content/drive/MyDrive/amhcd-data-64',\n","        '/content/drive/MyDrive/amhcd-data/amhcd-data-64',\n","        '/content/drive/MyDrive/Datasets/amhcd-data-64',\n","        # Ajoutez d'autres chemins possibles ici\n","    ]\n","\n","    for path in possible_paths:\n","        if os.path.exists(path):\n","            data_dir = path\n","            break\n","\n","    if not data_dir:\n","        print(\"Dossier non trouvé. Veuillez vérifier le chemin dans Google Drive\")\n","        print(\"Contenu complet de /content/drive/MyDrive :\")\n"],"metadata":{"id":"yx4I2h1YypQY"},"execution_count":null,"outputs":[]}]}